clc;
clear all;
close all;

%% 1. Generate training data (surface)

% Simple grid of x1 and x2 values
x1 = 0:0.25:1;   % 0, 0.25, 0.5, 0.75, 1  (5 points)
x2 = 0:0.25:1;   % 5 points
n1 = length(x1);
n2 = length(x2);

% Create grid of points (x1, x2)
[X1, X2] = meshgrid(x1, x2);

% Target surface: you can change this formula if your task says another
D = (1 + 0.6*sin(2*pi*X1/0.7) + 0.3*sin(2*pi*X2))/2;

% Plot target surface
figure(1);
mesh(X1, X2, D);
title('Target surface d(x_1, x_2)');
xlabel('x_1'); ylabel('x_2'); zlabel('d'); grid on;

%% 2. Initialize MLP parameters (2 inputs -> 4 hidden -> 1 output)

% Learning rate and number of epochs
eta   = 0.05;     % learning rate
epochs = 5000;    % fixed number of training epochs

% Random initial weights and biases
rng(0);           % for repeatable results

% Hidden neuron 1
w11_1 = randn(1);   % x1 -> hidden1
w21_1 = randn(1);   % x2 -> hidden1
b1_1  = randn(1);   % bias hidden1

% Hidden neuron 2
w12_1 = randn(1);   % x1 -> hidden2
w22_1 = randn(1);   % x2 -> hidden2
b2_1  = randn(1);

% Hidden neuron 3
w13_1 = randn(1);   % x1 -> hidden3
w23_1 = randn(1);   % x2 -> hidden3
b3_1  = randn(1);

% Hidden neuron 4
w14_1 = randn(1);   % x1 -> hidden4
w24_1 = randn(1);   % x2 -> hidden4
b4_1  = randn(1);

% Output neuron (linear): from 4 hidden neurons
w11_2 = randn(1);   % hidden1 -> output
w12_2 = randn(1);   % hidden2 -> output
w13_2 = randn(1);   % hidden3 -> output
w14_2 = randn(1);   % hidden4 -> output
b1_2  = randn(1);   % output bias

%% 3. Training (very basic backprop loop)

for ep = 1:epochs
    
    % Go through all training points
    for i = 1:n1
        for j = 1:n2
            
            % Current input (x1, x2) and desired output d
            x1_in = X1(j,i);
            x2_in = X2(j,i);
            d_ij  = D(j,i);
            
            %% --- Forward pass ---
            % Hidden layer net inputs
            v1_1 = x1_in*w11_1 + x2_in*w21_1 + b1_1;
            v2_1 = x1_in*w12_1 + x2_in*w22_1 + b2_1;
            v3_1 = x1_in*w13_1 + x2_in*w23_1 + b3_1;
            v4_1 = x1_in*w14_1 + x2_in*w24_1 + b4_1;
            
            % Sigmoid activations: y = 1/(1 + exp(-v))
            y1_1 = 1/(1 + exp(-v1_1));
            y2_1 = 1/(1 + exp(-v2_1));
            y3_1 = 1/(1 + exp(-v3_1));
            y4_1 = 1/(1 + exp(-v4_1));
            
            % Output neuron (linear)
            v1_2 = y1_1*w11_2 + y2_1*w12_2 + y3_1*w13_2 + y4_1*w14_2 + b1_2;
            y1_2 = v1_2;   % linear activation
            
            % Error
            e = d_ij - y1_2;
            
            %% --- Backpropagation ---
            % Output delta (linear activation -> derivative = 1)
            delta1_2 = e;
            
            % Hidden layer deltas (sigmoid derivative: y*(1-y))
            delta1_1 = y1_1*(1 - y1_1) * delta1_2 * w11_2;
            delta2_1 = y2_1*(1 - y2_1) * delta1_2 * w12_2;
            delta3_1 = y3_1*(1 - y3_1) * delta1_2 * w13_2;
            delta4_1 = y4_1*(1 - y4_1) * delta1_2 * w14_2;
            
            %% --- Weight and bias updates ---
            % Output neuron
            w11_2 = w11_2 + eta*delta1_2*y1_1;
            w12_2 = w12_2 + eta*delta1_2*y2_1;
            w13_2 = w13_2 + eta*delta1_2*y3_1;
            w14_2 = w14_2 + eta*delta1_2*y4_1;
            b1_2  = b1_2  + eta*delta1_2;
            
            % Hidden neuron 1
            w11_1 = w11_1 + eta*delta1_1*x1_in;
            w21_1 = w21_1 + eta*delta1_1*x2_in;
            b1_1  = b1_1  + eta*delta1_1;
            
            % Hidden neuron 2
            w12_1 = w12_1 + eta*delta2_1*x1_in;
            w22_1 = w22_1 + eta*delta2_1*x2_in;
            b2_1  = b2_1  + eta*delta2_1;
            
            % Hidden neuron 3
            w13_1 = w13_1 + eta*delta3_1*x1_in;
            w23_1 = w23_1 + eta*delta3_1*x2_in;
            b3_1  = b3_1  + eta*delta3_1;
            
            % Hidden neuron 4
            w14_1 = w14_1 + eta*delta4_1*x1_in;
            w24_1 = w24_1 + eta*delta4_1*x2_in;
            b4_1  = b4_1  + eta*delta4_1;
        end
    end
end

%% 4. Build approximation on a grid (testing)

x1_test = 0:0.05:1;
x2_test = 0:0.05:1;
[X1t, X2t] = meshgrid(x1_test, x2_test);

D_true   = (1 + 0.6*sin(2*pi*X1t/0.7) + 0.3*sin(2*pi*X2t))/2;
Y_approx = zeros(size(D_true));

for i = 1:length(x1_test)
    for j = 1:length(x2_test)
        x1_in = X1t(j,i);
        x2_in = X2t(j,i);
        
        % Forward pass only (no learning now)
        v1_1 = x1_in*w11_1 + x2_in*w21_1 + b1_1;
        v2_1 = x1_in*w12_1 + x2_in*w22_1 + b2_1;
        v3_1 = x1_in*w13_1 + x2_in*w23_1 + b3_1;
        v4_1 = x1_in*w14_1 + x2_in*w24_1 + b4_1;
        
        y1_1 = 1/(1 + exp(-v1_1));
        y2_1 = 1/(1 + exp(-v2_1));
        y3_1 = 1/(1 + exp(-v3_1));
        y4_1 = 1/(1 + exp(-v4_1));
        
        v1_2 = y1_1*w11_2 + y2_1*w12_2 + y3_1*w13_2 + y4_1*w14_2 + b1_2;
        y1_2 = v1_2;
        
        Y_approx(j,i) = y1_2;
    end
end

%% 5. Plot true surface vs MLP approximation

figure(2);
subplot(1,2,1);
mesh(X1t, X2t, D_true);
title('True surface');
xlabel('x_1'); ylabel('x_2'); zlabel('d(x_1,x_2)'); grid on;

subplot(1,2,2);
mesh(X1t, X2t, Y_approx);
title('MLP approximation');
xlabel('x_1'); ylabel('x_2'); zlabel('y_{MLP}(x_1,x_2)'); grid on;
